# Llama Modellerini KarÅŸÄ±laÅŸtÄ±rma

**GÃ¼ncelleme: Llama 3, 18 Nisan'da yayÄ±nlandÄ± ve bu not defteri, Together.ai tarafÄ±ndan barÄ±ndÄ±rÄ±lan Llama 3 ve Llama 2 modellerini karÅŸÄ±laÅŸtÄ±rmak iÃ§in gÃ¼ncellendi.**

Llama modellerini yÃ¶nlendirmek iÃ§in yardÄ±mcÄ± fonksiyonu yÃ¼kle:

```python
from utils import llama, llama_chat
```

**GÃ¶rev 1: Duygu SÄ±nÄ±flandÄ±rma** 
Modelleri birkaÃ§-atÄ±ÅŸ komut duygu sÄ±nÄ±flandÄ±rmasÄ±nda karÅŸÄ±laÅŸtÄ±rÄ±n. Modelden tek kelime ile yanÄ±t vermesini istiyorsunuz.

```python
prompt = '''
Mesaj: Merhaba Amit, dÃ¼ÅŸÃ¼nceli doÄŸum gÃ¼nÃ¼ kartÄ±n iÃ§in teÅŸekkÃ¼rler!
Duygu: Pozitif
Mesaj: Merhaba Baba, piyano resitalime 20 dakika geÃ§ kaldÄ±n!
Duygu: Negatif
Mesaj: Bu akÅŸam pizza sipariÅŸ etmek iÃ§in sabÄ±rsÄ±zlanÄ±yorum!
Duygu: ?
Tek kelime ile yanÄ±t ver.
'''
```

Ä°lk olarak, 7B parametreli sohbet modeli (llama-2-7b-chat) ile yanÄ±t alÄ±n. Together.ai tarafÄ±ndan kabul edilen model isimleri bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harfe duyarlÄ± deÄŸildir ve "META-LLAMA/LLAMA-2-7B-CHAT-HF" veya "togethercomputer/llama-2-7b-chat" olabilir. Åimdi "META-LLAMA" ile baÅŸlayan isimler tercih ediliyor.

```python
response = llama(prompt, model="META-LLAMA/Llama-2-7B-CHAT-HF")
print(response)
```

Åimdi, aynÄ± gÃ¶revde 70B parametreli sohbet modeli (llama-2-70b-chat) kullanÄ±n

```python
response = llama(prompt, model="META-LLAMA/Llama-2-70B-CHAT-HF")
print(response)
```

Llama 3 sohbet modellerini kullanma

```python
response = llama(prompt, model="META-LLAMA/Llama-3-8B-CHAT-HF")
print(response)
response = llama(prompt, model="META-LLAMA/Llama-3-70B-CHAT-HF")
print(response)
```

**GÃ¶rev 2: Ã–zetleme** 
Modelleri Ã¶zetleme gÃ¶revinde karÅŸÄ±laÅŸtÄ±rÄ±n. Bu, kursun Ã¶nceki bÃ¶lÃ¼mlerinde kullandÄ±ÄŸÄ±nÄ±z aynÄ± "email".

```python
email = """
Sevgili Amit,

BÃ¼yÃ¼k dil modelleri (LLMs) artÄ±k aÃ§Ä±k kaynaklÄ± veya neredeyse aÃ§Ä±k kaynaklÄ±. Nispeten serbest lisanslara sahip modellerin Ã§oÄŸalmasÄ±, geliÅŸtiricilere uygulamalar oluÅŸturmak iÃ§in daha fazla seÃ§enek sunuyor.

Ä°ÅŸte LLM'leri kullanarak uygulamalar oluÅŸturmanÄ±n bazÄ± farklÄ± yollarÄ±, maliyet/karmaÅŸÄ±klÄ±k sÄ±rasÄ±na gÃ¶re artarak:

TalimatlandÄ±rma. Ã–nceden eÄŸitilmiÅŸ bir LLM'ye talimat vermek, eÄŸitim seti olmadan dakikalar veya saatler iÃ§inde bir prototip oluÅŸturmanÄ±zÄ± saÄŸlar. Bu yÄ±lÄ±n baÅŸlarÄ±nda, birÃ§ok kiÅŸinin talimatlandÄ±rmayla denemeler yapmaya baÅŸladÄ±ÄŸÄ±nÄ± gÃ¶rdÃ¼m ve bu ivme kesintisiz devam ediyor. KÄ±sa kurslarÄ±mÄ±zdan birkaÃ§Ä±, bu yaklaÅŸÄ±m iÃ§in en iyi uygulamalarÄ± Ã¶ÄŸretiyor.

Tek atÄ±ÅŸ veya birkaÃ§ atÄ±ÅŸ talimatlandÄ±rma. Bir komutun yanÄ± sÄ±ra, LLM'ye bir gÃ¶revi nasÄ±l yapacaÄŸÄ±nÄ±n birkaÃ§ Ã¶rneÄŸini vermek â€” giriÅŸ ve istenen Ã§Ä±ktÄ± â€” bazen daha iyi sonuÃ§lar verir.

Ä°nce ayar. Ã‡ok miktarda metin Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸ bir LLM, kendi kÃ¼Ã§Ã¼k veri setiniz Ã¼zerinde daha fazla eÄŸitilerek gÃ¶rev

inize ince ayar yapÄ±labilir. Ä°nce ayar araÃ§larÄ± olgunlaÅŸÄ±yor, bu da onlarÄ± daha fazla geliÅŸtirici iÃ§in eriÅŸilebilir hale getiriyor.

Ã–n eÄŸitim. Kendi LLM'nizi sÄ±fÄ±rdan Ã¶n eÄŸitmek Ã§ok kaynak gerektirir, bu yÃ¼zden Ã§ok az takÄ±m bunu yapar. Ã‡eÅŸitli konularda Ã¶nceden eÄŸitilmiÅŸ genel amaÃ§lÄ± modellere ek olarak, bu yaklaÅŸÄ±m finans hakkÄ±nda bilgi sahibi olan BloombergGPT gibi ve tÄ±bba odaklanan Med-PaLM 2 gibi Ã¶zelleÅŸmiÅŸ modellerin ortaya Ã§Ä±kmasÄ±na yol aÃ§mÄ±ÅŸtÄ±r.

Ã‡oÄŸu takÄ±m iÃ§in, uygulamanÄ±zÄ± hÄ±zla Ã§alÄ±ÅŸÄ±r hale getirmenizi saÄŸladÄ±ÄŸÄ± iÃ§in talimatlandÄ±rmayla baÅŸlamanÄ±zÄ± Ã¶neririm. Ã‡Ä±ktÄ± kalitesinden memnun deÄŸilseniz, daha karmaÅŸÄ±k tekniklere yavaÅŸ yavaÅŸ geÃ§in. BirkaÃ§ Ã¶rnek ile tek atÄ±ÅŸ veya birkaÃ§ atÄ±ÅŸ talimatlandÄ±rmaya baÅŸlayÄ±n. Bu yeterince iyi Ã§alÄ±ÅŸmazsa, belki de LLM'nin yÃ¼ksek kaliteli Ã§Ä±ktÄ±lar Ã¼retmek iÃ§in gereken ana bilgilerle komutlarÄ± daha da iyileÅŸtirmek iÃ§in RAG (bilgi artÄ±rÄ±lmÄ±ÅŸ Ã¼retim) kullanÄ±n. Bu hala istediÄŸiniz performansÄ± saÄŸlamazsa, o zaman ince ayarÄ± deneyin â€” ancak bu, Ã¶nemli Ã¶lÃ§Ã¼de daha fazla karmaÅŸÄ±klÄ±k dÃ¼zeyini temsil eder ve yÃ¼zlerce veya binlerce Ã¶rnek daha gerektirebilir. Bu seÃ§enekleri derinlemesine anlamak iÃ§in, AWS ve DeepLearning.AI tarafÄ±ndan oluÅŸturulan BÃ¼yÃ¼k Dil Modelleri ile YaratÄ±cÄ± AI kursunu ÅŸiddetle tavsiye ederim.

(Ä°lginÃ§ bir gerÃ§ek: DeepLearning.AI ekibinden bir Ã¼ye, Llama-2-7B'yi benim gibi ses Ã§Ä±karmasÄ± iÃ§in ince ayar yapmaya Ã§alÄ±ÅŸÄ±yor. Acaba iÅŸim tehlikede mi? ğŸ˜œ)

Tescilli bir model olan ve ince ayara aÃ§Ä±k olmayan GPT-4 gibi bir modelden sonra ince ayara geÃ§mek istiyorsanÄ±z ek karmaÅŸÄ±klÄ±k ortaya Ã§Ä±kar. Ã‡ok daha kÃ¼Ã§Ã¼k bir modeli ince ayarlamak, daha bÃ¼yÃ¼k, daha yetenekli bir modeli talimatlandÄ±rmaktan daha Ã¼stÃ¼n sonuÃ§lar saÄŸlama olasÄ±lÄ±ÄŸÄ± var mÄ±? Cevap genellikle uygulamanÄ±za baÄŸlÄ±dÄ±r. Bir LLM'nin Ã§Ä±ktÄ± stilini deÄŸiÅŸtirmek amacÄ±nÄ±zsa, daha kÃ¼Ã§Ã¼k bir modeli ince ayarlamak iyi iÅŸleyebilir. Ancak, uygulamanÄ±z GPT-4'Ã¼ karmaÅŸÄ±k akÄ±l yÃ¼rÃ¼tme yapmasÄ± iÃ§in talimatlandÄ±rÄ±yorsa â€” GPT-4 mevcut aÃ§Ä±k modelleri aÅŸtÄ±ÄŸÄ± durumlarda â€” daha kÃ¼Ã§Ã¼k bir modeli Ã¼stÃ¼n sonuÃ§lar verecek ÅŸekilde ince ayarlamak zor olabilir.

Bir geliÅŸtirme yaklaÅŸÄ±mÄ± seÃ§menin Ã¶tesinde, belirli bir model seÃ§mek de gereklidir. Daha kÃ¼Ã§Ã¼k modeller daha az iÅŸlem gÃ¼cÃ¼ gerektirir ve birÃ§ok uygulama iÃ§in iyi Ã§alÄ±ÅŸÄ±r, ancak daha bÃ¼yÃ¼k modeller genellikle dÃ¼nya hakkÄ±nda daha fazla bilgiye sahiptir ve daha iyi akÄ±l yÃ¼rÃ¼tme yeteneÄŸine sahiptir. Bu seÃ§imi nasÄ±l yapacaÄŸÄ±nÄ±zÄ± ilerideki bir mektupta anlatacaÄŸÄ±m.

Ã–ÄŸrenmeye devam edin!

Andrew
"""

Ä°lk olarak, 7B parametreli sohbet modeli (llama-2-7b-chat) ile e-postayÄ± Ã¶zetleyin.

```python
response_7b = llama(prompt, model="META-LLAMA/Llama-2-7B-CHAT-HF")
print(response_7b)
```

Åimdi, 13B parametreli sohbet modeli (llama-2-13

b-chat) ile e-postayÄ± Ã¶zetleyin.

```python
response_13b = llama(prompt, model="META-LLAMA/Llama-2-13B-CHAT-HF")
print(response_13b)
```

Son olarak, 70B parametreli sohbet modeli (llama-2-70b-chat) ile e-postayÄ± Ã¶zetleyin.

```python
response_70b = llama(prompt, model="META-LLAMA/Llama-2-70B-CHAT-HF")
print(response_70b)
```

Llama 3 sohbet modellerini kullanma

```python
response_llama3_8b = llama(prompt, model="META-LLAMA/Llama-3-8B-CHAT-HF")
print(response_llama3_8b)
response_llama3_70b = llama(prompt, model="META-LLAMA/Llama-3-70B-CHAT-HF")
print(response_llama3_70b)
```
